{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will predict the next character based on previous characters in context. We will be using a LSTM model to incorporate the contextual dependencies in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference for this project is taken from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The dataset we are using is the book Alice in Wonderland. It is a fairly small corpus and hence, doesn't take much time in training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import all the dependencies.\n",
    "1. numpy- for mathematical calculations\n",
    "2. Sequential- create a Sequential model using keras\n",
    "3. layers- Various layers that we will be using to create our model(Dense, Dropout, LSTM)\n",
    "4. utils- to convert integer labels into categorical data(one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load our data. We will use the built in open function to open the file and read function to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('alice.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 100 characters in the file is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nALICE’S ADVENTURES IN WONDERLAND\\n\\nLewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\n\\n\\n\\nCHAPTER I.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to convert all the text to lowercase. We will use lower function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nalice’s adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\n\\n\\n\\nchapter i.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.lower()\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char_list is a list of all the unique characters present in the corpus. set function is use to eradicate duplicate entries and list is used to create a list of unique chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " ';',\n",
       " ' ',\n",
       " 'm',\n",
       " '“',\n",
       " '[',\n",
       " 'x',\n",
       " 'a',\n",
       " 'i',\n",
       " 'j',\n",
       " 'u',\n",
       " 'f',\n",
       " '.',\n",
       " ':',\n",
       " '”',\n",
       " '_',\n",
       " 'n',\n",
       " 's',\n",
       " 'r',\n",
       " 'l',\n",
       " '\\n',\n",
       " '!',\n",
       " 'k',\n",
       " 'h',\n",
       " 'v',\n",
       " 'p',\n",
       " 't',\n",
       " '*',\n",
       " '’',\n",
       " '0',\n",
       " ']',\n",
       " 'o',\n",
       " '3',\n",
       " 'g',\n",
       " ')',\n",
       " 'q',\n",
       " 'y',\n",
       " 'w',\n",
       " 'b',\n",
       " 'c',\n",
       " ',',\n",
       " 'z',\n",
       " '?',\n",
       " '-',\n",
       " 'd',\n",
       " 'e',\n",
       " '‘']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_list = list(set(data))\n",
    "char_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char is the list of characters in char_list arranged according to their precedence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = sorted(char_list)\n",
    "char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the built-in function enumerate() and dict() to make a dictionary of unique characters and their integer equivalents. Numeric data handling is much more useful compared to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '3': 10,\n",
       " ':': 11,\n",
       " ';': 12,\n",
       " '?': 13,\n",
       " '[': 14,\n",
       " ']': 15,\n",
       " '_': 16,\n",
       " 'a': 17,\n",
       " 'b': 18,\n",
       " 'c': 19,\n",
       " 'd': 20,\n",
       " 'e': 21,\n",
       " 'f': 22,\n",
       " 'g': 23,\n",
       " 'h': 24,\n",
       " 'i': 25,\n",
       " 'j': 26,\n",
       " 'k': 27,\n",
       " 'l': 28,\n",
       " 'm': 29,\n",
       " 'n': 30,\n",
       " 'o': 31,\n",
       " 'p': 32,\n",
       " 'q': 33,\n",
       " 'r': 34,\n",
       " 's': 35,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 38,\n",
       " 'w': 39,\n",
       " 'x': 40,\n",
       " 'y': 41,\n",
       " 'z': 42,\n",
       " '‘': 43,\n",
       " '’': 44,\n",
       " '“': 45,\n",
       " '”': 46}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(char))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we got 47(0 - 46) different integer equivalents corresponding to each character in the list char. The new dictionary is named char_to_int. It holds characters as key and integers as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  144414\n",
      "Total Vocab 47\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(data)\n",
    "n_vocab = len(char)\n",
    "print('Total characters: ', n_chars)\n",
    "print('Total Vocab', n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic processing on the dataset is done. We got a dictionary(char_to_int) of 47 unique characters and their integer equivalents. The total number of characters in the corpous is 144414."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to structure our dataset so that we can input it in our model. We will be keeping the sequence length of 200. That means all the characters(integer equivalent) from 0 to 199 will be used as the input and the 200th character will be out target value. Again, all the characters from 1 to 200 will be used as input and 201th character will be the target. This process will be repeated till we reach the end of file and this should give us (144414-200)=144214 size training input and corresponding number of labels. We will use an lstm model to retain the contextual meaning of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144214\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars-seq_length, 1):\n",
    "    seq_in = data[i:i+seq_length]\n",
    "    seq_out = data[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print('Total Patterns: ', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 values of the 0th entry in dataX:  [0, 0, 17, 28, 25, 19, 21, 44, 35, 1]\n",
      "The corresponding value of the 0th entry in dataY:  30\n"
     ]
    }
   ],
   "source": [
    "print('The first 10 values of the 0th entry in dataX: ', dataX[0][:10])\n",
    "print('The corresponding value of the 0th entry in dataY: ', dataY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, dataX contains 144214 rows and 200 columns. dataY contains 144214 rows and the corresponding target value for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before feeding the data is to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144214, 200, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X/float(n_vocab) #Normalizing the value of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.36170213],\n",
       "       [ 0.59574468],\n",
       "       [ 0.53191489],\n",
       "       [ 0.40425532],\n",
       "       [ 0.44680851],\n",
       "       [ 0.93617021],\n",
       "       [ 0.74468085],\n",
       "       [ 0.0212766 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X data is normalized to remove data redundancy and a few obtained values are shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, dataY is converted into its categorical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144214, 47)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create our sequential model using Keras and then fit it with data. We will use 200 lstm cells, dropout=0.3, activation=softmax and categorical_crossentropy loss with adam optimizer. The parameters chosen here are all arbitrary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_shape=(X.shape[1], X.shape[2]) because we have 144214 inputs with 200 features and 1 channel. This input is fed to the network and a dense layer with Y.shape[1] (=47) output nodes is obtained. Each node has its own softmax probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dimensions are all correct, the model is created. Now we need to feed the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using epoch size of 20 with batch_size=128. Again, these hyper-parameters are chosen at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 51712/144214 [=========>....................] - ETA: 1713s - loss: 3.0786"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The training step continued as shown above, for 20 epochs and loss is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a lstm model is successfully created which predicts the next character with substantial accuracy for a given input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
